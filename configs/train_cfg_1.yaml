output_dir: 'output'
checkpoint: 'unsloth/gemma-2-9b-it-bnb-4bit'
max_length: 1024
fold_idx: 0
optim_type: 'adamw_8bit'
per_device_train_batch_size: 2
gradient_accumulation_steps: 2
per_device_eval_batch_size: 8
n_epochs: 1
freeze_layers: 8
lr: 0.0002
warmup_steps: 20
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_bias: 'none'
dora: true
layers:
  - 'q_proj'
  - 'k_proj'
  - 'v_proj'
  - 'o_proj'
  - 'gate_proj'
  - 'up_proj'
  - 'down_proj'
num_labels: 50
sep: ' <sep> '
fillna: '{PASS}'
test_size: 0.1
